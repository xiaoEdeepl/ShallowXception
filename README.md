# DeepFake Detection Cross-Dataset Generalization Experiment

## ğŸ¯ **Experiment Objective**
**Compare the generalization capability between:**
- **Original Xception** 
- **Modified ShallowXception** (with Blocks 4-11 removed)

**Training Strategy:**
- ğŸ‹ï¸ **Train on:** FaceForensics++ (FF++) dataset
- ğŸ§ª **Test on:** DFDC dataset

## ğŸ“‚ **Dataset Architecture**

### Dataset Directory Structure
```text
dataset/
â”œâ”€â”€ dfdc/                    # DFDC test set
â”‚   â”œâ”€â”€ video_0              # 14% real samples,86% fake samples
â”‚   â”‚   â”œâ”€â”€frame1.jpg
â”‚   â”‚   â”œâ”€â”€frame2.jpg
â”‚   â”‚   â””â”€â”€...
â”‚   â”‚   
â”‚   â”œâ”€â”€ video_1              
â”‚   â”‚   â”œâ”€â”€frame1.jpg
â”‚   â”‚   â”œâ”€â”€frame2.jpg
â”‚   â”‚   â””â”€â”€...
â”‚   â”‚   
â”‚   â”œâ”€â”€ ... 
â”‚   â””â”€â”€ metadata.csv         # Ground truth labels
â”‚
â””â”€â”€ FF++/                    # FaceForensics++ training set
    â”œâ”€â”€ fake/                # 1000 manipulated videos per method
    â”‚   â”œâ”€â”€ df/              # Deepfakes
    â”‚   â”œâ”€â”€ f2f/             # Face2Face
    â”‚   â”œâ”€â”€ fshift/          # FaceShift
    â”‚   â”œâ”€â”€ fswap/           # FaceSwap 
    â”‚   â””â”€â”€ nt/              # NeuralTextures
    â”‚
    â””â”€â”€ real/                # 1000 original videos
```

## ğŸ”¬ **Experimental Design**

### 1. **Model Variants**
| Model          | Backbone Architecture      | Trainable Params | Pretrained |
|----------------|----------------------------|------------------|------------|
| Xception       | Original Xception          | 20.8M            | None       |
| ShallowXception| Xception (Blocks 4-11 removed) | -                | None       |

### 2. **Training Protocol**
- **Input:** 3x299x299 RGB frames (5 fps sampling from every fake video, 25 fps from every real video)
- **Optimizer:** Adam (lr=1e-4)

## ğŸ“Š **Preliminary Results (FF++ â†’ DFDC)**

### **Cross-Dataset Performance**
| Model          | ACC on FF++ (%) | ACC on DFDC (%) |
|----------------|-----------------|-----------------|
| Xception       | **98.7**        | 51.7            |
| ShallowXception| 94              | **62.5**        |

## âš¡ **Training**

### 1. **Basic Training Command**   
```bash  
  python train.py --model Xception --epoch 20 --bs 16 --lr 1e-4
```  
- `--model`: Select model type (`Xception` or `ShallowXception`)  
- `--epoch`: Total number of epochs  
- `--bs`: Batch size  
- `--lr`: Initial learning rate  

### 2. **Arguments**  
|  Parameter |  Type |  Default | Description                                          |  
|----------------|----------|---------------|------------------------------------------------------|  
| `--c`          |    -     |     -         | Resume training from checkpoint                      |  
| `--epoch`      | int      | 10            | Number of epochs                                     |  
| `--model`      | str      | "xception"    | Model selection <br> `xception` or `shallowxception` |  
| `--bs`         | int      | 8             | Batch size                                           |  
| `--lr`         | float    | 1e-4          | Learning rate                                        |  
| `--v`          |    -     |     -         | Enable t-SNE visualization                           |
|`--dataset`     |  str     |    ff         |train dataset (ff, dfdc, cdf)                         |

### 3. **Output Files**   
The script generates the following files:  
1. **Model weights**: Saved in `./weight/` as `<model_name>.pth`  
2. **Training logs**: Saved in `./log/` as `<model_name>_logs.csv`  
3. **Loss/Accuracy plots**: Saved in `./figures/` with timestamps  
4. **t-SNE visualization (optional)**: Saved in `./figures/`  

